import streamlit as st
import google.generativeai as genai
import os
from dotenv import load_dotenv
from PIL import Image
from streamlit_mic_recorder import mic_recorder
from gtts import gTTS
import io
import re

# 1. Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØµÙØ­Ø©
st.set_page_config(page_title="Ù…Ø³Ø§Ø¹Ø¯ Ù…ØµØ¹Ø¨ Ø§Ù„Ø°ÙƒÙŠ - Ù†Ø³Ø®Ø© Ø§Ù„ØµÙˆØ±", layout="wide", page_icon="ğŸ–¼ï¸")
load_dotenv()

# 2. Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…ÙØªØ§Ø­
api_key = os.getenv("GEMINI_API_KEY")
genai.configure(api_key=api_key)

# 3. Ø¯Ø§Ù„Ø© Ø§Ù„ØªÙˆÙ„ÙŠØ¯ (Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø© ÙÙŠ Ø­Ø³Ø§Ø¨Ùƒ)
def smart_generate(contents):
    models = ["gemini-3-flash-preview", "gemini-2.0-flash-exp", "gemini-1.5-flash"]
    for m in models:
        try:
            model = genai.GenerativeModel(m)
            response = model.generate_content(contents)
            return response.text, m
        except:
            continue
    return "ğŸš« Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø§ØªØµØ§Ù„.", None

# --- Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© ---
st.title("âš¡ Ù…Ø³Ø§Ø¹Ø¯ Ù…ØµØ¹Ø¨ Ø§Ù„Ù…ØªÙƒØ§Ù…Ù„")

if "messages" not in st.session_state:
    st.session_state.messages = []

with st.sidebar:
    audio_record = mic_recorder(start_prompt="ØªØ­Ø¯Ø« ğŸ¤", stop_prompt="Ø¥Ø±Ø³Ø§Ù„ ğŸ“¤", key='recorder')
    uploaded_file = st.file_uploader("Ø§Ø±ÙØ¹ ØµÙˆØ±Ø©:", type=["jpg", "png", "jpeg"])

# Ø¹Ø±Ø¶ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# Ø§Ø³ØªÙ‚Ø¨Ø§Ù„ Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª
user_input = st.chat_input("Ø§Ø·Ù„Ø¨ Ø±Ø³Ù… ØµÙˆØ±Ø© Ø£Ùˆ Ø§Ø³Ø£Ù„ Ø³Ø¤Ø§Ù„Ø§Ù‹...")
current_audio = audio_record['bytes'] if audio_record else None

if user_input or current_audio or uploaded_file:
    prompt = user_input if user_input else "Ø­Ù„Ù„ Ù‡Ø°Ø§"
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        with st.spinner("Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªÙˆÙ„ÙŠØ¯..."):
            content_list = [prompt]
            if uploaded_file: content_list.append(Image.open(uploaded_file))
            if current_audio: content_list.append({"mime_type": "audio/wav", "data": current_audio})
            
            raw_answer, used_model = smart_generate(content_list)
            
            if used_model:
                # --- Ø§Ù„Ø³Ø­Ø± Ù‡Ù†Ø§: Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø±Ø§Ø¨Ø· Ø§Ù„ØµÙˆØ±Ø© ÙˆØ¹Ø±Ø¶Ù‡ ÙÙˆØ±Ø§Ù‹ ---
                # Ù‡Ø°Ø§ Ø§Ù„Ø¬Ø²Ø¡ ÙŠØ¨Ø­Ø« Ø¹Ù† Ø£ÙŠ Ø±Ø§Ø¨Ø· ÙŠØ¨Ø¯Ø£ Ø¨Ù€ http ÙˆÙŠÙ†ØªÙ‡ÙŠ Ø¨ØµÙŠØºØ© ØµÙˆØ±Ø©
                img_match = re.search(r'(https?://\S+?\.(?:png|jpg|jpeg|gif))', raw_answer)
                
                if img_match:
                    image_url = img_match.group(1)
                    st.image(image_url, caption="ØªÙ… ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØµÙˆØ±Ø© Ø¨Ù†Ø¬Ø§Ø­!")
                
                # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Øµ Ù…Ù† Ø£ÙƒÙˆØ§Ø¯ Ø§Ù„Ù€ JSON Ø§Ù„Ù…Ø²Ø¹Ø¬Ø© Ù„Ù„Ø¹Ø±Ø¶ ÙÙ‚Ø·
                clean_text = re.sub(r'\{.*?\}', 'ğŸ¨ Ø¬Ø§Ø±ÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø·Ù„Ø¨ Ø§Ù„ØµÙˆØ±Ø©...', raw_answer, flags=re.DOTALL)
                
                st.markdown(clean_text)
                st.caption(f"ğŸ¤– Ø§Ù„Ù…Ø­Ø±Ùƒ: {used_model}")
                
                # Ø§Ù„Ø±Ø¯ Ø§Ù„ØµÙˆØªÙŠ
                try:
                    tts = gTTS(text=clean_text[:200], lang='ar')
                    fp = io.BytesIO()
                    tts.write_to_fp(fp)
                    st.audio(fp, autoplay=True)
                except: pass
                
                st.session_state.messages.append({"role": "assistant", "content": clean_text})
