import streamlit as st
import google.generativeai as genai
from openai import OpenAI
import io, re, os, subprocess
from gtts import gTTS
from PIL import Image
from streamlit_mic_recorder import mic_recorder 

# --- 1. Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª ÙˆØ§Ù„ÙˆØ§Ø¬Ù‡Ø© (RTL) ---
st.set_page_config(page_title="Ù…Ù†ØµØ© Ù…ØµØ¹Ø¨ v16.11.9", layout="wide", page_icon="ğŸ¤")

st.markdown("""
    <style>
    .stApp { direction: rtl; text-align: right; }
    section[data-testid="stSidebar"] { direction: rtl; text-align: right; background-color: #111; }
    .stSelectbox label, .stSlider label { color: #00ffcc !important; font-weight: bold; }
    </style>
    """, unsafe_allow_html=True)

# Ø§Ù„Ø±Ø¨Ø· Ø§Ù„Ù…Ø­Ù„ÙŠ (LM Studio) Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©
# Ø§Ù„Ø¹Ù†ÙˆØ§Ù†: http://127.0.0.1:1234 ÙƒÙ…Ø§ ÙŠØ¸Ù‡Ø± ÙÙŠ Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬
local_client = OpenAI(base_url="http://127.0.0.1:1234/v1", api_key="lm-studio")

api_key = st.secrets.get("GEMINI_API_KEY")
if api_key:
    genai.configure(api_key=api_key)

# --- 2. Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠØ© Ø§Ù„Ø´Ø§Ù…Ù„Ø© ---
with st.sidebar:
    st.header("ğŸ® Ù…Ø±ÙƒØ² Ø§Ù„ØªØ­ÙƒÙ… v16.11.9")
    
    st.subheader("ğŸ¤ Ø§Ù„Ù…ØºØ±ÙÙˆÙ† (Ù„Ù„ØªÙƒÙ„Ù…)")
    audio_record = mic_recorder(
        start_prompt="Ø¨Ø¯Ø¡ Ø§Ù„ØªØ³Ø¬ÙŠÙ„", 
        stop_prompt="Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„ØµÙˆØª", 
        just_once=True, 
        key='sidebar_mic'
    )
    
    st.divider()

    thinking_level = st.select_slider(
        "ğŸ§  Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØªÙÙƒÙŠØ±:", 
        options=["Low", "Medium", "High"], 
        value="High"
    )
    
    persona = st.selectbox(
        "ğŸ‘¤ Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø®Ø¨ÙŠØ±:", 
        ["Ø§Ù„Ù…Ø¹Ø±ÙÙˆÙ† (Ø£Ù‡Ù„ Ø§Ù„Ø¹Ù„Ù…)", "Ø®Ø¨ÙŠØ± Ø§Ù„Ù„ØºØ§Øª", "ÙˆÙƒÙŠÙ„ ØªÙ†ÙÙŠØ°ÙŠ", "Ù…Ø³Ø§Ø¹Ø¯ Ù…Ø¨Ø±Ù…Ø¬"]
    )
    
    st.divider()
    
    engine_choice = st.selectbox(
        "ğŸ¯ Ø§Ù„Ù…Ø­Ø±Ùƒ:",
        ["DeepSeek R1 (Local)", "Gemini 2.5 Flash", "Gemini 3 Pro"]
    )
    
    uploaded_file = st.file_uploader("ğŸ“‚ Ø±ÙØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª:", type=["pdf", "csv", "txt", "jpg", "png", "jpeg"])

# --- 3. ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø¯Ø±Ø¯Ø´Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© ---
if "messages" not in st.session_state: 
    st.session_state.messages = []

# Ø¹Ø±Ø¶ Ø§Ù„Ø±Ø³Ø§Ø¦Ù„ Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]): 
        st.markdown(msg["content"])

# --- Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ù…Ø¯Ù…Ø¬ Ù„Ø·Ù„Ø¨ Ø§Ù„Ø±Ø¯ (Ù…Ø­Ø¯Ø« Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ ØµÙˆØ±ØªÙƒ) ---
if prompt := st.chat_input("Ø§Ø³Ø£Ù„ Ø°ÙƒØ§Ø¡Ùƒ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„Ù…Ø­Ù„ÙŠ..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        try:
            # Ø§Ù„Ù…Ø¹Ø±Ù Ù…Ø£Ø®ÙˆØ° Ù…Ù† "API Model Identifier" ÙÙŠ ØµÙˆØ±ØªÙƒ
            stream = local_client.chat.completions.create(
                model="deepseek-r1-distill-qwen-1.5b", 
                messages=[{"role": "user", "content": prompt}],
                stream=True
            )
            answer = st.write_stream(stream)
            st.session_state.messages.append({"role": "assistant", "content": answer})
        except Exception as e:
            st.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø§ØªØµØ§Ù„: ØªØ£ÙƒØ¯ Ø£Ù† LM Studio Ù„Ø§ ÙŠØ²Ø§Ù„ Ù‚ÙŠØ¯ Ø§Ù„ØªØ´ØºÙŠÙ„. {e}")
