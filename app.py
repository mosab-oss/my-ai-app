import streamlit as st
from google import genai
from google.genai import types
from openai import OpenAI

# --- [1] Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù‡ÙˆÙŠØ© ÙˆØ§Ù„Ø¬Ù…Ø§Ù„ÙŠØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ---
st.set_page_config(page_title="Ø¥Ù…Ø¨Ø±Ø§Ø·ÙˆØ±ÙŠØ© Ø§Ù„ØªØ­Ø§Ù„Ù 2026", layout="wide", page_icon="ğŸ›ï¸")

st.markdown("""
    <style>
    @import url('https://fonts.googleapis.com/css2?family=Cairo:wght@400;700&display=swap');
    html, body, [class*="css"] { font-family: 'Cairo', sans-serif; text-align: right; direction: rtl; }
    .stChatFloatingInputContainer { direction: ltr; }
    .stSidebar { background-color: #1e1e1e; color: white; }
    </style>
    """, unsafe_allow_html=True)

# --- [2] Ù…Ø¬Ù…Ø¹ Ø§Ù„Ø¹Ù‚ÙˆÙ„ Ø§Ù„Ø³ÙŠØ§Ø¯ÙŠØ© (Ø¨Ø¯ÙˆÙ† Ø§Ù„ØµÙŠÙ†ÙŠ) ---
model_map = {
    "ğŸ‡ºğŸ‡¸ Gemini 2.0 (Ø¬ÙˆØ¬Ù„ - Ù„Ù„Ø¨Ø­Ø« Ø§Ù„Ø­ÙŠ)": "models/gemini-1.5-flash",
    "ğŸ‡ºğŸ‡¸ Claude 3.5 (Ø£Ù†ØªØ«Ø±ÙˆØ¨ÙŠÙƒ - Ù„Ù„Ø¯Ù‚Ø© Ø§Ù„Ù„ØºÙˆÙŠØ©)": "anthropic/claude-3.5-sonnet",
    "ğŸ‡ªğŸ‡º Mistral Large (Ø£ÙˆØ±ÙˆØ¨Ø§ - Ù„Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ù†Ø·Ù‚ÙŠ)": "mistralai/mistral-large"
}

# --- [3] Ù…Ø¬Ù„Ø³ Ø§Ù„Ù…Ø³ØªØ´Ø§Ø±ÙŠÙ† ÙˆØ§Ù„Ù…Ø¯Ø±Ø¨ÙŠÙ† ---
expert_map = {
    "ğŸ›¡ï¸ Ø§Ù„Ù…Ø³ØªØ´Ø§Ø± Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠ": "Ø£Ù†Øª Ø¬Ù†Ø±Ø§Ù„ ÙˆÙ…Ø­Ù„Ù„ Ø¬ÙŠÙˆØ³ÙŠØ§Ø³ÙŠØŒ ØªØ­Ù„Ù„ Ø§Ù„Ù…ÙˆØ§Ù‚Ù Ø¨Ø±Ø¤ÙŠØ© Ù‚ÙŠØ§Ø¯ÙŠØ© Ø¹Ø³ÙƒØ±ÙŠØ© ÙˆØªØ§Ø±ÙŠØ®ÙŠØ©.",
    "ğŸ“ˆ Ø§Ù„Ù…Ø­Ù„Ù„ Ø§Ù„Ù…Ø§Ù„ÙŠ": "Ø£Ù†Øª Ø®Ø¨ÙŠØ± Ø£Ø³ÙˆØ§Ù‚ Ø¹Ø§Ù„Ù…ÙŠØŒ ØªØªØ§Ø¨Ø¹ Ø§Ù„Ø°Ù‡Ø¨ ÙˆØ§Ù„Ø¨ÙˆØ±ØµØ© ÙˆØªØ³ØªØ®Ø¯Ù… Ø±Ø§Ø¯Ø§Ø± Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù„Ø­Ø¸ÙŠ.",
    "ğŸ‘¨â€ğŸ« Ø§Ù„Ø¨Ø±ÙˆÙÙŠØ³ÙˆØ± Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠ": "Ø£Ù†Øª Ù…Ø¯Ø±Ø³ Ù‚Ø¯ÙŠØ±ØŒ ØªØ´Ø±Ø­ Ø£Ø¹Ù‚Ø¯ Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ… Ø¨ØªØ¨Ø³ÙŠØ· Ù…Ø°Ù‡Ù„ ÙˆÙ„ØºØ© Ø¹Ø±Ø¨ÙŠØ© Ø³Ù„ÙŠÙ…Ø©.",
    "âš–ï¸ Ø§Ù„Ù…Ø³ØªØ´Ø§Ø± Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ": "Ø£Ù†Øª ÙÙ‚ÙŠÙ‡ Ù‚Ø§Ù†ÙˆÙ†ÙŠØŒ ØªØ±Ø§Ø¬Ø¹ Ø§Ù„Ø¹Ù‚ÙˆØ¯ ÙˆØ§Ù„Ø£Ù†Ø¸Ù…Ø© Ø¨Ø°ÙƒØ§Ø¡ ÙˆØ­Ø±Øµ Ø´Ø¯ÙŠØ¯.",
    "ğŸ’» Ø§Ù„Ø®Ø¨ÙŠØ± Ø§Ù„ØªÙ‚Ù†ÙŠ": "Ø£Ù†Øª ÙƒØ¨ÙŠØ± Ù…Ù‡Ù†Ø¯Ø³ÙŠÙ†ØŒ ÙˆØ¸ÙŠÙØªÙƒ Ø­Ù„ Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠØ© ÙˆØªØµÙ…ÙŠÙ… Ø§Ù„Ø£Ù†Ø¸Ù…Ø©."
}

# --- [4] Ù…Ø­Ø±Ùƒ Ø§Ù„Ø§Ø³ØªØ´Ø§Ø±Ø© Ø§Ù„Ù…Ø±ÙƒØ²ÙŠ ---
def run_council_engine(user_query, model_name, expert_role):
    system_instr = expert_map[expert_role]
    
    try:
        if "Gemini" in model_name:
            client = genai.Client(api_key=st.secrets["GEMINI_API_KEY"])
            response = client.models.generate_content(
                model="gemini-1.5-flash",
                contents=user_query,
                config=types.GenerateContentConfig(
                    system_instruction=system_instr,
                    tools=[types.Tool(google_search=types.GoogleSearch())]
                )
            )
            return response.text
        else:
            # Ø¹Ù‚ÙˆÙ„ OpenRouter (Claude / Mistral)
            client = OpenAI(base_url="https://openrouter.ai/api/v1", api_key=st.secrets["OPENROUTER_API_KEY"])
            res = client.chat.completions.create(
                model=model_map[model_name],
                messages=[
                    {"role": "system", "content": system_instr},
                    {"role": "user", "content": user_query}
                ]
            )
            return res.choices[0].message.content
    except Exception as e:
        return f"ğŸš¨ Ø¹Ø°Ø±Ø§Ù‹ ÙŠØ§ Ù‚Ø§Ø¦Ø¯ØŒ Ø§Ù„Ù…Ø³ØªØ´Ø§Ø± Ù…Ø´ØºÙˆÙ„ Ø­Ø§Ù„ÙŠØ§Ù‹: {str(e)}"

# --- [5] ÙˆØ§Ø¬Ù‡Ø© Ù…Ø±ÙƒØ² Ø§Ù„Ù‚ÙŠØ§Ø¯Ø© ---
st.title("ğŸ›ï¸ Ø¥Ù…Ø¨Ø±Ø§Ø·ÙˆØ±ÙŠØ© Ø§Ù„ØªØ­Ø§Ù„Ù: Ù…Ø¬Ù„Ø³ Ø§Ù„Ø®Ø¨Ø±Ø§Ø¡")

with st.sidebar:
    st.header("ğŸ‘¤ Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù…Ø³ØªØ´Ø§Ø±")
    selected_expert = st.selectbox("Ù…Ù† ØªØ±ÙŠØ¯ Ø§Ø³ØªØ´Ø§Ø±ØªÙ‡ØŸ", list(expert_map.keys()))
    
    st.divider()
    st.header("ğŸ§  Ø§Ù„Ø¹Ù‚Ù„ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬")
    selected_model = st.radio("Ø§Ø®ØªØ± Ø§Ù„Ù…Ø­Ø±Ùƒ Ø§Ù„Ø°ÙƒÙŠ:", list(model_map.keys()))
    
    st.divider()
    st.success("âœ… Ø§Ù„Ù†Ø¸Ø§Ù… ÙŠØ¹Ù…Ù„ Ø¨Ø£Ø±Ù‚Ù‰ Ø§Ù„Ø¹Ù‚ÙˆÙ„ Ø§Ù„Ø£Ù…Ø±ÙŠÙƒÙŠØ© ÙˆØ§Ù„Ø£ÙˆØ±ÙˆØ¨ÙŠØ©.")

# Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø°Ø§ÙƒØ±Ø© ÙˆØ³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
if "messages" not in st.session_state:
    st.session_state.messages = []

for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# Ø¥Ø¯Ø®Ø§Ù„ Ø§Ù„Ø£ÙˆØ§Ù…Ø±
if prompt := st.chat_input("ØªØ­Ø¯Ø« Ù…Ø¹ Ø§Ù„Ù…Ø³ØªØ´Ø§Ø±..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        with st.spinner(f"Ø¬Ø§Ø±ÙŠ ØªØ­Ø¶ÙŠØ± {selected_expert}..."):
            answer = run_council_engine(prompt, selected_model, selected_expert)
            st.markdown(answer)
            st.session_state.messages.append({"role": "assistant", "content": answer})
