import streamlit as st
import google.generativeai as genai
from openai import OpenAI
import io, re, os, subprocess
from gtts import gTTS
from PIL import Image
from streamlit_mic_recorder import mic_recorder 

# --- 1. Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª ÙˆØ§Ù„ÙˆØ§Ø¬Ù‡Ø© (RTL) ---
st.set_page_config(page_title="Ù…Ù†ØµØ© Ù…ØµØ¹Ø¨ v16.11.4", layout="wide", page_icon="ğŸ™ï¸")

st.markdown("""
    <style>
    .stApp { direction: rtl; text-align: right; }
    code, pre { direction: ltr !important; text-align: left !important; display: block; }
    section[data-testid="stSidebar"] { direction: rtl; text-align: right; }
    </style>
    """, unsafe_allow_html=True)

# Ø§Ù„Ø±Ø¨Ø· Ø§Ù„Ù…Ø­Ù„ÙŠ ÙˆÙ…Ø­Ø±ÙƒØ§Øª Ø¬ÙˆØ¬Ù„
local_client = OpenAI(base_url="http://127.0.0.1:1234/v1", api_key="lm-studio")
api_key = st.secrets.get("GEMINI_API_KEY")
if api_key:
    genai.configure(api_key=api_key)

# --- 2. Ù…Ø±ÙƒØ² Ø§Ù„ØªØ­ÙƒÙ… ---
with st.sidebar:
    st.header("ğŸ® Ù…Ø±ÙƒØ² Ø§Ù„ØªØ­ÙƒÙ… v16.11.4")
    
    engine_choice = st.selectbox(
        "ğŸ¯ Ø§Ø®ØªØ± Ø§Ù„Ù…Ø­Ø±Ùƒ:",
        ["Gemini 2.5 Flash", "Gemini 3 Pro", "Gemma 3 27B", "DeepSeek R1 (Ù…Ø­Ù„ÙŠ)"]
    )
    
    persona = st.selectbox(
        "ğŸ‘¤ Ø§Ø®ØªØ± Ø§Ù„Ø®Ø¨ÙŠØ± Ø§Ù„Ù…Ø·Ù„ÙˆØ¨:", 
        ["Ø§Ù„Ù…ØºØ±ÙÙˆÙ† (Ø®Ø¨ÙŠØ± Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ø¹Ø§Ù…)", "Ø®Ø¨ÙŠØ± Ø§Ù„Ù„ØºØ§Øª ÙˆØ§Ù„ØªØ±Ø¬Ù…Ø©", "ÙˆÙƒÙŠÙ„ ØªÙ†ÙÙŠØ° Ù…Ù„ÙØ§Øª", "Ù…Ø³Ø§Ø¹Ø¯ Ù…Ø¨Ø±Ù…Ø¬ Ù…Ø­ØªØ±Ù"]
    )
    
    st.divider()
    uploaded_file = st.file_uploader("ğŸ“‚ Ø§Ø±ÙØ¹ Ù…Ù„ÙÙƒ:", type=["pdf", "csv", "txt", "jpg", "png", "jpeg"])
    
    # Ù…ÙŠØ²Ø© Ø§Ù„ØªØ³Ø¬ÙŠÙ„ Ø§Ù„ØµÙˆØªÙŠ Ù„Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø£ÙˆØ§Ù…Ø± Ø¨Ø§Ù„ØµÙˆØª
    st.subheader("ğŸ™ï¸ ØªØ­Ø¯Ø« Ø¥Ù„Ù‰ Ø§Ù„Ù…Ù†ØµØ©")
    audio_record = mic_recorder(start_prompt="ğŸ¤ Ø§Ø¨Ø¯Ø£ Ø§Ù„ØªØ³Ø¬ÙŠÙ„", stop_prompt="ğŸ›‘ Ø¥Ø±Ø³Ø§Ù„", just_once=True, key='mic_input')

# --- 3. Ø¯Ø§Ù„Ø© Ø§Ù„ØªÙ†ÙÙŠØ° Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ ---
def clean_and_execute(text):
    cleaned = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL).strip()
    file_pattern = r'(?:SAVE_FILE:|save_file:)\s*([\w\.-]+)\s*(?:\||content=\{?)\s*(.*?)\s*\}?$'
    match = re.search(file_pattern, cleaned, flags=re.IGNORECASE | re.DOTALL)
    if match:
        filename, content = match.group(1).strip(), match.group(2).strip()
        content = re.sub(r'```python|```', '', content).strip()
        try:
            with open(filename, 'w', encoding='utf-8') as f: f.write(content)
            if filename.endswith('.py'):
                res = subprocess.run(['python3', filename], capture_output=True, text=True, timeout=10)
                return cleaned + f"\n\nâœ… **ØªÙ… Ø§Ù„ØªÙ†ÙÙŠØ°!** \n Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª: \n `{res.stdout}`"
            return cleaned + f"\n\nâœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù: `{filename}`"
        except Exception as e: return cleaned + f"\n\nâŒ Ø®Ø·Ø£: {e}"
    return cleaned

# --- 4. ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø¯Ø±Ø¯Ø´Ø© ÙˆØ§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØªÙŠØ© ---
if "messages" not in st.session_state: st.session_state.messages = []
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]): st.markdown(msg["content"])

prompt = st.chat_input("Ø§Ø³Ø£Ù„ Ø§Ù„Ù…ØºØ±ÙÙˆÙ† Ø´ÙŠØ¦Ø§Ù‹...")

# Ø¯Ù…Ø¬ Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª (Ù†Øµ Ø£Ùˆ ØµÙˆØª)
if prompt or audio_record or uploaded_file:
    user_txt = prompt if prompt else "ğŸ“‚ [ØªØ­Ù„ÙŠÙ„ Ù…Ù„Ù Ù…Ø±ÙÙ‚]"
    st.session_state.messages.append({"role": "user", "content": user_txt})
    with st.chat_message("user"): st.markdown(user_txt)

    with st.chat_message("assistant"):
        full_res = ""
        system_instructions = {
            "Ø§Ù„Ù…ØºØ±ÙÙˆÙ† (Ø®Ø¨ÙŠØ± Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ø¹Ø§Ù…)": "Ø£Ù†Øª Ø®Ø¨ÙŠØ± Ù…ÙˆØ³ÙˆØ¹ÙŠ Ù…ØªØ­Ø¯Ø«. Ù‚Ø¯Ù… Ø¥Ø¬Ø§Ø¨Ø§Øª Ø«Ø±ÙŠØ© ÙˆÙ…Ø¹Ø±ÙÙŠØ© Ø¨Ø£Ø³Ù„ÙˆØ¨ Ù„Ø¨Ù‚.",
            "Ø®Ø¨ÙŠØ± Ø§Ù„Ù„ØºØ§Øª ÙˆØ§Ù„ØªØ±Ø¬Ù…Ø©": "Ø£Ù†Øª Ø®Ø¨ÙŠØ± Ù„ØºÙˆÙŠØŒ Ø±ÙƒØ² Ø¹Ù„Ù‰ Ø¯Ù‚Ø© Ø§Ù„Ù…Ø®Ø§Ø±Ø¬ ÙˆØ§Ù„ØªØ±Ø¬Ù…Ø©.",
            "ÙˆÙƒÙŠÙ„ ØªÙ†ÙÙŠØ° Ù…Ù„ÙØ§Øª": "Ø£Ù†Øª ÙˆÙƒÙŠÙ„ ØªÙ‚Ù†ÙŠ Ù„ØªÙ†ÙÙŠØ° Ø§Ù„Ø£ÙƒÙˆØ§Ø¯.",
            "Ù…Ø³Ø§Ø¹Ø¯ Ù…Ø¨Ø±Ù…Ø¬ Ù…Ø­ØªØ±Ù": "Ø£Ù†Øª Ù…Ø¨Ø±Ù…Ø¬ Ø®Ø¨ÙŠØ±."
        }
        
        instruction = system_instructions.get(persona, "")
        
        try:
            model_map = {"Gemini 3 Pro": "models/gemini-3-pro-preview", "Gemini 2.5 Flash": "models/gemini-2.5-flash", "Gemma 3 27B": "models/gemma-3-27b-it"}
            model = genai.GenerativeModel(model_map.get(engine_choice, "models/gemini-2.5-flash"))
            
            # Ø§Ù„Ø·Ù„Ø¨
            response = model.generate_content(f"{instruction}\n\n{user_txt}")
            full_res = clean_and_execute(response.text)
            st.markdown(full_res)
            
            # --- Ù…ÙŠØ²Ø© Ø§Ù„ØªÙƒÙ„Ù… (Text-to-Speech) ---
            # Ù†Ù‚ÙˆÙ… Ø¨ØªØ­ÙˆÙŠÙ„ Ø£ÙˆÙ„ 300 Ø­Ø±Ù Ù…Ù† Ø±Ø¯ "Ø§Ù„Ù…ØºØ±ÙÙˆÙ†" Ø¥Ù„Ù‰ ØµÙˆØª
            clean_audio_text = re.sub(r'[*#`]', '', full_res) # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Øµ Ù…Ù† Ø§Ù„Ø±Ù…ÙˆØ² Ù„ÙŠÙƒÙˆÙ† Ø§Ù„ØµÙˆØª Ø£ÙˆØ¶Ø­
            tts = gTTS(text=clean_audio_text[:300], lang='ar')
            audio_fp = io.BytesIO()
            tts.write_to_fp(audio_fp)
            st.audio(audio_fp, format='audio/mp3')
            
            st.session_state.messages.append({"role": "assistant", "content": full_res})
        except Exception as e: st.error(f"Ø­Ø¯Ø« Ø®Ø·Ø£: {e}")
