import streamlit as st
import pandas as pd
from openai import OpenAI
import io
import speech_recognition as sr
from pydub import AudioSegment
from gtts import gTTS
from streamlit_mic_recorder import mic_recorder 

# --- 1. Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª ÙˆØ§Ù„ÙˆØ§Ø¬Ù‡Ø© ---
st.set_page_config(page_title="Ù…Ù†ØµØ© Ù…ØµØ¹Ø¨ v16.11.9", layout="wide")

st.markdown("""
    <style>
    .stApp { direction: rtl; text-align: right; }
    section[data-testid="stSidebar"] { direction: rtl; text-align: right; background-color: #111; }
    .stStatusWidget { direction: rtl; } /* ØªØ­Ø³ÙŠÙ† Ù…Ø¸Ù‡Ø± Ø´Ø±ÙŠØ· Ø§Ù„Ø­Ø§Ù„Ø© */
    </style>
    """, unsafe_allow_html=True)

# Ø§Ù„Ø±Ø¨Ø· Ø¨Ù€ LM Studio
local_client = OpenAI(base_url="http://127.0.0.1:1234/v1", api_key="lm-studio")

# --- 2. ÙˆØ¸Ø§Ø¦Ù Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø¹ Ø¥Ø¸Ù‡Ø§Ø± Ø§Ù„Ø­Ø§Ù„Ø© ---

def transcribe_audio_fixed(audio_bytes):
    r = sr.Recognizer()
    try:
        audio = AudioSegment.from_file(io.BytesIO(audio_bytes))
        wav_io = io.BytesIO()
        audio.export(wav_io, format="wav")
        wav_io.seek(0)
        with sr.AudioFile(wav_io) as source:
            audio_data = r.record(source)
            # Ø¥Ø¸Ù‡Ø§Ø± Ù†Ù…Ø· "Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ø§Ù…"
            return r.recognize_google(audio_data, language='ar-SA')
    except Exception as e:
        return f"Ø®Ø·Ø£: {str(e)}"

# --- 3. Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø°Ø§ÙƒØ±Ø© ---
if "messages" not in st.session_state:
    st.session_state.messages = []

# --- 4. Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠØ© ---
with st.sidebar:
    st.header("ğŸ® Ù…Ø±ÙƒØ² Ø§Ù„ØªØ­ÙƒÙ…")
    if st.button("ğŸ—‘ï¸ Ù…Ø³Ø­ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©"):
        st.session_state.messages = []
        st.rerun()

    st.divider()
    st.subheader("ğŸ¤ Ø§Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ†")
    audio_record = mic_recorder(start_prompt="ØªØ­Ø¯Ø« Ø§Ù„Ø¢Ù†", stop_prompt="Ø¥Ø±Ø³Ø§Ù„", key='mic')
    
    st.divider()
    # Ø¥Ø¸Ù‡Ø§Ø± Ø­Ø§Ù„Ø© Ø§Ù„Ù…Ø­Ø±Ùƒ ÙÙŠ Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠØ©
    st.success("Ø§Ù„Ù…Ø­Ø±Ùƒ Ø§Ù„Ù…Ø­Ù„ÙŠ: Ù…ØªØµÙ„" if local_client else "Ø§Ù„Ù…Ø­Ø±Ùƒ Ø§Ù„Ù…Ø­Ù„ÙŠ: ØºÙŠØ± Ù…ØªØµÙ„")

# --- 5. ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø¯Ø±Ø¯Ø´Ø© ---
for msg in st.session_state.messages:
    if msg["role"] != "system":
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])

# --- 6. Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„ ÙˆØ¥Ø¸Ù‡Ø§Ø± "Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø°ÙƒØ§Ø¡" ---
prompt = st.chat_input("Ø§ÙƒØªØ¨ Ø³Ø¤Ø§Ù„Ùƒ Ù‡Ù†Ø§...")
user_input = None

if audio_record:
    # Ù†Ù…Ø· 1: Ø¬Ø§Ø±ÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØª
    with st.status("ğŸ¤ Ø¬Ø§Ø±ÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© ØµÙˆØªÙƒ ÙˆØªØ­ÙˆÙŠÙ„Ù‡ Ù„Ù†Øµ...", expanded=True) as status:
        user_input = transcribe_audio_fixed(audio_record['bytes'])
        status.update(label="âœ… ØªÙ… ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØª Ø¨Ù†Ø¬Ø§Ø­!", state="complete", expanded=False)
elif prompt:
    user_input = prompt

if user_input:
    st.session_state.messages.append({"role": "user", "content": user_input})
    with st.chat_message("user"):
        st.markdown(user_input)

    with st.chat_message("assistant"):
        # Ù†Ù…Ø· 2: Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªÙÙƒÙŠØ± (Thinking Pattern)
        with st.spinner("ğŸ§  Ø°ÙƒØ§Ø¡ DeepSeek ÙŠÙÙƒØ± Ø§Ù„Ø¢Ù† ÙÙŠ Ø§Ù„Ø±Ø¯..."):
            try:
                system_instruction = "Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ ØªØ¬ÙŠØ¨ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙÙ‚Ø·."
                messages_to_send = [{"role": "system", "content": system_instruction}]
                messages_to_send.extend([{"role": m["role"], "content": m["content"]} for m in st.session_state.messages])

                stream = local_client.chat.completions.create(
                    model="deepseek-r1-distill-qwen-1.5b",
                    messages=messages_to_send,
                    stream=True,
                    temperature=0.3
                )
                # Ù†Ù…Ø· 3: Ø¬Ø§Ø±ÙŠ Ø§Ù„ÙƒØªØ§Ø¨Ø© (Streaming Pattern)
                answer = st.write_stream(stream)
                st.session_state.messages.append({"role": "assistant", "content": answer})
                
                # Ù†Ù…Ø· 4: Ø¬Ø§Ø±ÙŠ Ø§Ù„Ù†Ø·Ù‚ Ø§Ù„ØµÙˆØªÙŠ
                with st.toast("ğŸ”Š Ø¬Ø§Ø±ÙŠ ØªØ´ØºÙŠÙ„ Ø§Ù„Ø±Ø¯ Ø§Ù„ØµÙˆØªÙŠ..."):
                    tts = gTTS(text=answer[:400], lang='ar')
                    audio_fp = io.BytesIO()
                    tts.write_to_fp(audio_fp)
                    st.audio(audio_fp, format='audio/mp3')
            
            except Exception as e:
                st.error(f"Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ Ù†Ù…Ø· Ø§Ù„Ø§ØªØµØ§Ù„: {e}")
