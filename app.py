import streamlit as st
import google.generativeai as genai
from openai import OpenAI
import io, re, os, subprocess
from gtts import gTTS
from PIL import Image
from streamlit_mic_recorder import mic_recorder 

# --- 1. Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª ÙˆØ§Ù„ÙˆØ§Ø¬Ù‡Ø© (RTL) ---
st.set_page_config(page_title="Ù…Ù†ØµØ© Ù…ØµØ¹Ø¨ v16.11.0", layout="wide", page_icon="ğŸ“")

st.markdown("""
    <style>
    .stApp { direction: rtl; text-align: right; }
    code, pre { direction: ltr !important; text-align: left !important; display: block; }
    section[data-testid="stSidebar"] { direction: rtl; text-align: right; }
    </style>
    """, unsafe_allow_html=True)

# Ø§Ù„Ø±Ø¨Ø· Ø§Ù„Ù…Ø­Ù„ÙŠ (DeepSeek)
local_client = OpenAI(base_url="http://127.0.0.1:1234/v1", api_key="lm-studio")

# Ø±Ø¨Ø· Ù…Ø­Ø±ÙƒØ§Øª Ø¬ÙˆØ¬Ù„
api_key = st.secrets.get("GEMINI_API_KEY")
if api_key:
    genai.configure(api_key=api_key)

# --- 2. Ù…Ø±ÙƒØ² Ø§Ù„ØªØ­ÙƒÙ… Ø§Ù„Ù…Ø·ÙˆØ± ---
with st.sidebar:
    st.header("ğŸ® Ù…Ø±ÙƒØ² Ø§Ù„ØªØ­ÙƒÙ… v16.11.0")
    
    engine_choice = st.selectbox(
        "ğŸ¯ Ø§Ø®ØªØ± Ø§Ù„Ù…Ø­Ø±Ùƒ:",
        ["Gemini 2.5 Flash", "Gemini 3 Pro", "Gemma 3 27B", "DeepSeek R1 (Ù…Ø­Ù„ÙŠ)"]
    )
    
    # --- Ø§Ù„Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© Ù‡Ù†Ø§: Ø§Ù„Ø´Ø®ØµÙŠØ§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© ---
    persona = st.selectbox(
        "ğŸ‘¤ Ø§Ø®ØªØ± Ø§Ù„Ø®Ø¨ÙŠØ± Ø§Ù„Ù…Ø·Ù„ÙˆØ¨:", 
        [
            "ÙˆÙƒÙŠÙ„ ØªÙ†ÙÙŠØ° Ù…Ù„ÙØ§Øª", 
            "Ø§Ù„Ù…Ø¹Ø±ÙÙˆÙ† (Ø®Ø¨ÙŠØ± Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ø¹Ø§Ù…)", 
            "Ø®Ø¨ÙŠØ± Ø§Ù„Ù„ØºØ§Øª ÙˆØ§Ù„ØªØ±Ø¬Ù…Ø©", 
            "Ù…Ø³Ø§Ø¹Ø¯ Ù…Ø¨Ø±Ù…Ø¬ Ù…Ø­ØªØ±Ù"
        ]
    )
    
    thinking_level = st.select_slider("ğŸ§  Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØªÙÙƒÙŠØ±:", options=["Low", "Medium", "High"], value="High")
    
    st.divider()
    uploaded_file = st.file_uploader("ğŸ“‚ Ø§Ø±ÙØ¹ Ù…Ù„ÙÙƒ:", type=["pdf", "csv", "txt", "jpg", "png", "jpeg"])
    
    # Ø£Ø¯ÙˆØ§Øª Ø§Ù„ØµÙŠØ§Ù†Ø© (Ø²Ø± Ø§Ù„ÙØ­Øµ)
    st.subheader("ğŸ› ï¸ Ø£Ø¯ÙˆØ§Øª Ø§Ù„ØµÙŠØ§Ù†Ø©")
    if st.button("ğŸ” ÙØ­Øµ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„Ø§Øª Ø§Ù„Ù†Ø´Ø·Ø©"):
        try:
            models = [m.name for m in genai.list_models() if 'generateContent' in m.supported_generation_methods]
            st.info("Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø© Ù„Ø­Ø³Ø§Ø¨Ùƒ Ø­Ø§Ù„ÙŠØ§Ù‹:")
            st.code("\n".join(models))
        except Exception as e: st.error(f"ÙØ´Ù„ Ø§Ù„ÙØ­Øµ: {e}")

# --- 3. Ù…Ø­Ø±Ùƒ Ø§Ù„Ø£ÙˆØ§Ù…Ø± (Ø§Ù„ÙˆÙƒÙŠÙ„ Ø§Ù„Ø°ÙƒÙŠ) ---
def clean_and_execute(text):
    cleaned = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL).strip()
    file_pattern = r'(?:SAVE_FILE:|save_file:)\s*([\w\.-]+)\s*(?:\||content=\{?)\s*(.*?)\s*\}?$'
    match = re.search(file_pattern, cleaned, flags=re.IGNORECASE | re.DOTALL)
    
    if match:
        filename, content = match.group(1).strip(), match.group(2).strip()
        content = re.sub(r'```python|```', '', content).strip()
        try:
            with open(filename, 'w', encoding='utf-8') as f: f.write(content)
            if filename.endswith('.py'):
                res = subprocess.run(['python3', filename], capture_output=True, text=True, timeout=10)
                output = res.stdout if res.stdout else res.stderr
                return cleaned + f"\n\n--- \n âœ… **ØªÙ… Ø§Ù„ØªÙ†ÙÙŠØ°!** \n\n**Ø§Ù„Ù†ØªÙŠØ¬Ø©:** \n ``` \n {output} \n ```"
            return cleaned + f"\n\n--- \n âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù `{filename}`."
        except Exception as e: return cleaned + f"\n\n--- \n âŒ Ø®Ø·Ø£ Ù†Ø¸Ø§Ù…: {e}"
    return cleaned

# --- 4. ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø¯Ø±Ø¯Ø´Ø© ---
if "messages" not in st.session_state: st.session_state.messages = []
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]): st.markdown(msg["content"])

prompt = st.chat_input("ØªØ­Ø¯Ø« Ù…Ø¹ Ø®Ø¨ÙŠØ±Ùƒ...")

if prompt or uploaded_file:
    # ØµÙŠØ§ØºØ© Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø´Ø®ØµÙŠØ© Ø§Ù„Ù…Ø®ØªØ§Ø±Ø©
    system_instructions = {
        "Ø§Ù„Ù…Ø¹Ø±ÙÙˆÙ† (Ø®Ø¨ÙŠØ± Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ø¹Ø§Ù…)": "Ø£Ù†Øª Ø®Ø¨ÙŠØ± Ù…ÙˆØ³ÙˆØ¹ÙŠØŒ Ù‚Ø¯Ù… ØªØ¹Ø±ÙŠÙØ§Øª Ø¹Ù…ÙŠÙ‚Ø©ØŒ Ø­Ù‚Ø§Ø¦Ù‚ ØªØ§Ø±ÙŠØ®ÙŠØ©ØŒ ÙˆØ´Ø±ÙˆØ­Ø§Øª Ø¹Ù„Ù…ÙŠØ© Ø¯Ù‚ÙŠÙ‚Ø©.",
        "Ø®Ø¨ÙŠØ± Ø§Ù„Ù„ØºØ§Øª ÙˆØ§Ù„ØªØ±Ø¬Ù…Ø©": "Ø£Ù†Øª Ø¨Ø±ÙˆÙÙŠØ³ÙˆØ± Ù„ØºÙˆÙŠØ§ØªØŒ Ù…ØªØ®ØµØµ ÙÙŠ Ø§Ù„ØªØ±Ø¬Ù…Ø© Ø¨ÙŠÙ† Ø§Ù„Ù„ØºØ§ØªØŒ ØªØµØ­ÙŠØ­ Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯ØŒ ÙˆØ´Ø±Ø­ Ø§Ù„Ù…ØµØ·Ù„Ø­Ø§Øª Ø§Ù„Ù…Ø¹Ù‚Ø¯Ø©.",
        "ÙˆÙƒÙŠÙ„ ØªÙ†ÙÙŠØ° Ù…Ù„ÙØ§Øª": "Ø£Ù†Øª ÙˆÙƒÙŠÙ„ ØªÙ‚Ù†ÙŠØŒ Ù…Ù‡Ù…ØªÙƒ ÙƒØªØ§Ø¨Ø© Ø§Ù„Ø£ÙƒÙˆØ§Ø¯ ÙˆØªÙ†ÙÙŠØ°Ù‡Ø§ ÙˆØ­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØµÙŠØºØ© SAVE_FILE.",
        "Ù…Ø³Ø§Ø¹Ø¯ Ù…Ø¨Ø±Ù…Ø¬ Ù…Ø­ØªØ±Ù": "Ø£Ù†Øª Ù…Ø¨Ø±Ù…Ø¬ Ø®Ø¨ÙŠØ±ØŒ Ø±ÙƒØ² Ø¹Ù„Ù‰ ÙƒÙØ§Ø¡Ø© Ø§Ù„ÙƒÙˆØ¯ØŒ Ø´Ø±Ø­ Ø§Ù„Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§ØªØŒ ÙˆØ­Ù„ Ø§Ù„Ù…Ø´ÙƒÙ„Ø§Øª Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠØ©."
    }
    
    instruction = system_instructions.get(persona, "")
    user_txt = prompt if prompt else "ğŸ“‚ [ØªØ­Ù„ÙŠÙ„ Ù…Ø±ÙÙ‚]"
    st.session_state.messages.append({"role": "user", "content": user_txt})
    
    with st.chat_message("user"): st.markdown(user_txt)

    with st.chat_message("assistant"):
        full_res = ""
        
        # Ù…Ø­Ø±ÙƒØ§Øª Ø¬ÙˆØ¬Ù„
        if "Gemini" in engine_choice or "Gemma" in engine_choice:
            try:
                model_map = {
                    "Gemini 3 Pro": "models/gemini-3-pro-preview",
                    "Gemini 2.5 Flash": "models/gemini-2.5-flash",
                    "Gemma 3 27B": "models/gemma-3-27b-it"
                }
                model = genai.GenerativeModel(model_map.get(engine_choice))
                
                # Ø¯Ù…Ø¬ Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª Ù…Ø¹ Ø·Ù„Ø¨ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
                full_prompt = f"{instruction}\n\nØ·Ù„Ø¨ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: {prompt}"
                
                response = model.generate_content(full_prompt)
                full_res = clean_and_execute(response.text)
                st.markdown(full_res)
            except Exception as e: st.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù…Ø­Ø±Ùƒ: {e}")

        # Ù…Ø­Ø±Ùƒ DeepSeek Ø§Ù„Ù…Ø­Ù„ÙŠ
        elif "DeepSeek" in engine_choice:
            try:
                # (ÙƒÙˆØ¯ DeepSeek Ø§Ù„Ù…ÙƒØªÙ…Ù„ Ù…Ù† v16.10.1)
                pass

        if full_res:
            st.session_state.messages.append({"role": "assistant", "content": full_res})
