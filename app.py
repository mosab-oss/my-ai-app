import streamlit as st
import google.generativeai as genai
from openai import OpenAI
import io, re
from gtts import gTTS
from PIL import Image

# --- 1. Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª ÙˆØ§Ù„Ø±Ø¨Ø· ---
st.set_page_config(page_title="Ù…Ù†ØµØ© Ù…ØµØ¹Ø¨ V16.2", layout="wide", page_icon="ğŸ’")

# Ø±Ø¨Ø· Ø§Ù„Ù…Ø­Ø±Ùƒ Ø§Ù„Ù…Ø­Ù„ÙŠ (LM Studio)
local_client = OpenAI(base_url="http://localhost:1234/v1", api_key="lm-studio")

# Ø±Ø¨Ø· Ù…Ø­Ø±ÙƒØ§Øª Ø¬ÙˆØ¬Ù„
api_key = st.secrets.get("GEMINI_API_KEY")
if api_key:
    genai.configure(api_key=api_key)

# --- 2. Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠØ© (Ø¥Ø¶Ø§ÙØ© Gemma 3) ---
with st.sidebar:
    st.header("ğŸ® Ù…Ø±ÙƒØ² Ø§Ù„ØªØ­ÙƒÙ… v16.2")
    
    # Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…Ø­Ø¯Ø«Ø© Ù„Ù„Ù…Ø­Ø±ÙƒØ§Øª
    engine_choice = st.selectbox(
        "ğŸ¯ Ø§Ø®ØªØ± Ø§Ù„Ø¹Ù‚Ù„ Ø§Ù„Ù…Ø¯Ø¨Ø±:",
        [
            "DeepSeek R1 (Ù…Ø­Ù„ÙŠ - Offline)", 
            "Gemini 2.5 Flash (Ø§Ù„Ø£Ø³Ø±Ø¹)", 
            "Gemma 3 27B IT (Ø§Ù„ÙˆØ³Ø· Ø§Ù„Ø°ÙƒÙŠ)", # Ø§Ù„Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©
            "Gemini 3 Pro (Ø§Ù„Ø£Ø°ÙƒÙ‰)", 
            "Imagen 3 (ØªÙˆÙ„ÙŠØ¯ ØµÙˆØ±)"
        ]
    )
    
    # Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„Ø§Øª Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠØ© Ø§Ù„Ø¯Ù‚ÙŠÙ‚Ø©
    model_map = {
        "Gemini 2.5 Flash (Ø§Ù„Ø£Ø³Ø±Ø¹)": "gemini-2.5-flash-exp",
        "Gemma 3 27B IT (Ø§Ù„ÙˆØ³Ø· Ø§Ù„Ø°ÙƒÙŠ)": "gemma-3-27b-it", # ØªÙ… Ø§Ù„ØªØ­Ø¯ÙŠØ« Ù‡Ù†Ø§
        "Gemini 3 Pro (Ø§Ù„Ø£Ø°ÙƒÙ‰)": "gemini-3-pro-preview"
    }
    
    st.divider()
    
    persona = st.selectbox("ğŸ‘¤ Ø´Ø®ØµÙŠØ© Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯:", ["Ù…Ø³Ø§Ø¹Ø¯ Ù…Ø¨Ø±Ù…Ø¬", "Ø®Ø¨ÙŠØ± Ù„ØºÙˆÙŠ", "Ù…Ø­Ù„Ù„ Ø°ÙƒÙŠ"])
    uploaded_file = st.file_uploader("ğŸ“¸ Ø±ÙØ¹ ØµÙˆØ±Ø© Ù„Ù„ØªØ­Ù„ÙŠÙ„:", type=["jpg", "png", "jpeg"])
    
    if st.button("ğŸ—‘ï¸ Ù…Ø³Ø­ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©"):
        st.session_state.messages = []
        st.rerun()

# --- 3. Ø§Ù„Ø¯ÙˆØ§Ù„ Ø§Ù„Ø°ÙƒÙŠØ© ---
def clean_think_tags(text):
    return re.sub(r'<think>.*?</think>', '', text, flags=st.DOTALL).strip()

# --- 4. ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø¯Ø±Ø¯Ø´Ø© ---
if "messages" not in st.session_state:
    st.session_state.messages = []

for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(clean_think_tags(msg["content"]))

# --- 5. Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø·Ù„Ø¨Ø§Øª ---
if prompt := st.chat_input("ØªØ­Ø¯Ø« Ù…Ø¹ Ø§Ù„Ù…Ø­Ø±Ùƒ Ø§Ù„Ù…Ø®ØªØ§Ø±..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        full_response = ""
        
        # Ø£. ÙˆØ¶Ø¹ DeepSeek Ø§Ù„Ù…Ø­Ù„ÙŠ
        if "DeepSeek" in engine_choice:
            try:
                res = local_client.chat.completions.create(
                    model="deepseek-r1-distill-qwen-7b",
                    messages=[{"role": "system", "content": f"Ø£Ù†Øª {persona}"}, {"role": "user", "content": prompt}],
                    stream=True
                )
                placeholder = st.empty()
                for chunk in res:
                    if chunk.choices[0].delta.content:
                        full_response += chunk.choices[0].delta.content
                        placeholder.markdown(full_response + "â–Œ")
                placeholder.markdown(clean_think_tags(full_response))
            except:
                st.error("ØªØ£ÙƒØ¯ Ù…Ù† ØªØ´ØºÙŠÙ„ Ø§Ù„Ø³ÙŠØ±ÙØ± Ø§Ù„Ù…Ø­Ù„ÙŠ!")

        # Ø¨. ÙˆØ¶Ø¹ Ù…Ø­Ø±ÙƒØ§Øª Ø¬ÙˆØ¬Ù„ (Gemini & Gemma)
        elif any(name in engine_choice for name in ["Gemini", "Gemma"]):
            try:
                selected_model = model_map[engine_choice]
                model = genai.GenerativeModel(selected_model)
                
                if uploaded_file:
                    img = Image.open(uploaded_file)
                    res = model.generate_content([prompt, img])
                else:
                    res = model.generate_content(prompt)
                
                full_response = res.text
                st.markdown(full_response)
            except Exception as e:
                st.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø§ØªØµØ§Ù„: {e}")

        # Ø¬. ÙˆØ¶Ø¹ Ø§Ù„ØµÙˆØª Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ
        if full_response:
            audio_text = clean_think_tags(full_response)
            try:
                tts = gTTS(text=audio_text[:300], lang='ar')
                audio_io = io.BytesIO()
                tts.write_to_fp(audio_io)
                st.audio(audio_io)
            except: pass
            
            st.session_state.messages.append({"role": "assistant", "content": full_response})
