import streamlit as st
import google.generativeai as genai
import os
from dotenv import load_dotenv
from PIL import Image
from streamlit_mic_recorder import mic_recorder
from gtts import gTTS
import io

# 1. Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØµÙØ­Ø© ÙˆØ§Ù„Ø¨ÙŠØ¦Ø©
st.set_page_config(page_title="Ù…ØµØ¹Ø¨ AI - Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø©", layout="wide", page_icon="âš¡")
load_dotenv()

# 2. Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù€ Google AI
api_key = os.getenv("GEMINI_API_KEY")
if not api_key:
    st.error("âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ GEMINI_API_KEY ÙÙŠ Ù…Ù„Ù .env")
    st.stop()

genai.configure(api_key=api_key)

# 3. Ø¯Ø§Ù„Ø© Ø§Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø°ÙƒÙŠØ© (Fallback Mechanism)
# Ù‡Ø°Ù‡ Ø§Ù„Ø¯Ø§Ù„Ø© ØªØ¶Ù…Ù† Ø¹Ø¯Ù… ØªÙˆÙ‚Ù Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø¥Ø°Ø§ Ø§Ù†ØªÙ‡Øª Ø­ØµØ© Gemini 3
def smart_generate(contents):
    # ØªØ±ØªÙŠØ¨ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„Ø§Øª: Ù†Ø¨Ø¯Ø£ Ø¨Ø§Ù„Ø£Ù‚ÙˆÙ‰ Ø«Ù… Ø§Ù„Ø£ÙƒØ«Ø± ØªÙˆÙØ±Ø§Ù‹
    models_to_try = ["gemini-3-flash", "gemini-2.0-flash", "gemini-1.5-flash"]
    
    for model_name in models_to_try:
        try:
            model = genai.GenerativeModel(model_name)
            response = model.generate_content(contents)
            return response.text, model_name
        except Exception as e:
            # Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø®Ø·Ø£ Ø¨Ø³Ø¨Ø¨ Ø§Ù„Ø­ØµØ© (Quota 429)ØŒ Ø§Ù†ØªÙ‚Ù„ Ù„Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø§Ù„ØªØ§Ù„ÙŠ
            if "429" in str(e) or "quota" in str(e).lower():
                continue 
            else:
                return f"âš ï¸ Ø®Ø·Ø£ ØªÙ‚Ù†ÙŠ: {e}", None
    return "ğŸš« Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø§Ù†ØªÙ‡Øª Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø­ØµØµ Ø§Ù„Ù…Ø¬Ø§Ù†ÙŠØ© Ù„Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„Ø§Øª Ø§Ù„ÙŠÙˆÙ….", None

# 4. Ø¯Ø§Ù„Ø© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ØµÙˆØª (Ø§Ù„Ù†Ø·Ù‚)
def speak(text):
    try:
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Øµ Ù…Ù† Ø§Ù„Ø±Ù…ÙˆØ² Ù„ÙŠÙƒÙˆÙ† Ø§Ù„Ù†Ø·Ù‚ Ø·Ø¨ÙŠØ¹ÙŠØ§Ù‹
        clean_text = text.replace('*', '').replace('#', '')
        tts = gTTS(text=clean_text[:300], lang='ar') # Ù†Ø·Ù‚ Ø£ÙˆÙ„ 300 Ø­Ø±Ù Ù„Ø³Ø±Ø¹Ø© Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©
        fp = io.BytesIO()
        tts.write_to_fp(fp)
        return fp
    except:
        return None

# 5. ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠØ© (Ø§Ù„Ø£Ø¯ÙˆØ§Øª)
with st.sidebar:
    st.header("ğŸ¨ Ø£Ø¯ÙˆØ§Øª Ø§Ù„ØªØ­ÙƒÙ…")
    
    # Ù…ÙŠØ²Ø© Ø§Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ†
    st.subheader("ğŸ™ï¸ ØªØ³Ø¬ÙŠÙ„ ØµÙˆØªÙŠ")
    audio_record = mic_recorder(
        start_prompt="Ø¥Ø¶ØºØ· Ù„Ù„ØªØ­Ø¯Ø« ğŸ¤",
        stop_prompt="Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„ØµÙˆØª ğŸ“¤",
        key='recorder'
    )
    
    st.divider()
    
    # Ù…ÙŠØ²Ø© Ø§Ù„Ø±Ø¤ÙŠØ© (Vision)
    st.subheader("ğŸ–¼ï¸ Ø±ÙØ¹ ØµÙˆØ±")
    uploaded_file = st.file_uploader("Ø§Ø®ØªØ± ØµÙˆØ±Ø© Ù„ØªØ­Ù„ÙŠÙ„Ù‡Ø§:", type=["jpg", "png", "jpeg"])
    
    if st.button("ğŸ—‘ï¸ Ù…Ø³Ø­ Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©"):
        st.session_state.messages = []
        st.rerun()

# 6. Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
st.title("âš¡ Ù…Ø³Ø§Ø¹Ø¯ Ù…ØµØ¹Ø¨ Ø§Ù„Ù…ØªÙƒØ§Ù…Ù„")
st.info("Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ ÙŠØªÙ†Ù‚Ù„ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹ Ø¨ÙŠÙ† Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„Ø§Øª Ù„Ø¶Ù…Ø§Ù† Ø§Ø³ØªÙ…Ø±Ø§Ø± Ø§Ù„Ø®Ø¯Ù…Ø©.")

if "messages" not in st.session_state:
    st.session_state.messages = []

# Ø¹Ø±Ø¶ Ø§Ù„Ø±Ø³Ø§Ø¦Ù„ Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# Ø§Ø³ØªÙ‚Ø¨Ø§Ù„ Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª (Ù†ØµÙŠØŒ ØµÙˆØªÙŠØŒ Ø£Ùˆ ØµÙˆØ±Ø©)
user_input = st.chat_input("Ø§ÙƒØªØ¨ Ø±Ø³Ø§Ù„ØªÙƒ Ù‡Ù†Ø§...")
current_audio = audio_record['bytes'] if audio_record else None

if user_input or current_audio or uploaded_file:
    # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù†Øµ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ Ù„Ù„Ø·Ù„Ø¨
    prompt = user_input if user_input else "Ø­Ù„Ù„ Ø§Ù„Ù…Ø±ÙÙ‚Ø§Øª"
    
    with st.chat_message("user"):
        st.markdown(prompt)
        if uploaded_file: st.image(uploaded_file, width=300)
        if current_audio: st.audio(current_audio)

    with st.chat_message("assistant"):
        with st.spinner("Ø¬Ø§Ø±ÙŠ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©..."):
            # ØªØ¬Ù‡ÙŠØ² Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…Ø­ØªÙˆÙŠØ§Øª Ù„Ù€ Gemini
            content_list = [prompt]
            if uploaded_file:
                content_list.append(Image.open(uploaded_file))
            if current_audio:
                # Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„ØµÙˆØª ÙƒÙ…ØµÙÙˆÙØ© Ø¨ÙŠØ§Ù†Ø§Øª
                content_list.append({"mime_type": "audio/wav", "data": current_audio})
            
            # ØªÙ†ÙÙŠØ° Ø§Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø°ÙƒÙŠ
            response_text, model_used = smart_generate(content_list)
            
            if model_used:
                st.markdown(response_text)
                st.caption(f"ğŸ¤– ØªÙ… Ø§Ù„Ø±Ø¯ Ø¨ÙˆØ§Ø³Ø·Ø©: {model_used}")
                
                # ØªØ´ØºÙŠÙ„ Ø§Ù„Ø±Ø¯ Ø§Ù„ØµÙˆØªÙŠ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹
                audio_fp = speak(response_text)
                if audio_fp:
                    st.audio(audio_fp, autoplay=True)
                
                # Ø­ÙØ¸ ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø©
                st.session_state.messages.append({"role": "assistant", "content": response_text})
            else:
                st.error(response_text)
