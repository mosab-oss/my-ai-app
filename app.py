import streamlit as st
import os
import io
import base64
from google import genai
from google.genai import types
from openai import OpenAI
from PIL import Image
import arabic_reshaper
from bidi.algorithm import get_display
import fitz  # PyMuPDF Ù„Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù€ PDF
from gtts import gTTS
from streamlit_mic_recorder import mic_recorder

# --- [1] Ù…Ø¬Ù…Ø¹ Ø§Ù„Ø¹Ù‚ÙˆÙ„ Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠØ© Ø§Ù„Ù…ÙˆØ­Ø¯ ---
model_map = {
    "ğŸ‡ºğŸ‡¸ Gemini 2.0 Flash (Ø¬ÙˆØ¬Ù„)": "models/gemini-2.0-flash-exp",
    "ğŸ‡¨ğŸ‡³ DeepSeek R1 (Ø§Ù„ØµÙŠÙ† - Ø§Ù„ØªÙÙƒÙŠØ±)": "deepseek/deepseek-r1",
    "ğŸ‡¨ğŸ‡³ Qwen 2.5 (Ø§Ù„ØµÙŠÙ† - Ø§Ù„Ù…Ø¹Ø±ÙØ©)": "qwen/qwen-2.5-72b-instruct",
    "ğŸ‡ªğŸ‡º Mistral Large (Ø£ÙˆØ±ÙˆØ¨Ø§)": "mistralai/mistral-large",
    "ğŸ‡ºğŸ‡¸ Claude 3.5 Sonnet (Ø£Ù…Ø±ÙŠÙƒØ§)": "anthropic/claude-3.5-sonnet"
}

expert_map = {
    "ğŸ“œ Ø§Ù„Ø¨Ø±ÙˆÙÙŠØ³ÙˆØ± Ø§Ù„Ù„ØºÙˆÙŠ Ø§Ù„Ø¹Ø±Ø¨ÙŠ": "Ø£Ù†Øª Ù…Ø±Ø¬Ø¹ ÙÙŠ Ø§Ù„Ø¨Ù„Ø§ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©ØŒ ÙˆØ¸ÙŠÙØªÙƒ ØµÙŠØ§ØºØ© Ø§Ù„Ø±Ø¯ÙˆØ¯ Ø¨Ø£ÙØµØ­ Ø¨ÙŠØ§Ù†.",
    "ğŸ›¡ï¸ Ø§Ù„Ù…Ø­Ù„Ù„ Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠ": "Ø£Ù†Øª Ø®Ø¨ÙŠØ± Ø¬ÙŠÙˆØ³ÙŠØ§Ø³ÙŠ ÙˆØ¹Ø³ÙƒØ±ÙŠØŒ ØªØ­Ù„Ù„ Ø§Ù„Ù‚ÙˆÙ‰ Ø§Ù„Ø¹Ø¸Ù…Ù‰ Ø¨Ø¯Ù‚Ø©.",
    "ğŸ“ˆ Ø®Ø¨ÙŠØ± Ø§Ù„Ø£Ø³ÙˆØ§Ù‚ Ø§Ù„Ø¯ÙˆÙ„ÙŠØ©": "Ø£Ù†Øª Ù…Ø­Ù„Ù„ Ù…Ø§Ù„ÙŠØŒ Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø­ÙŠ Ù„Ø¬Ù„Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø°Ù‡Ø¨ ÙˆØ§Ù„Ø¨ÙˆØ±ØµØ©.",
    "ğŸ’» ÙƒØ¨ÙŠØ± Ø§Ù„Ù…Ù‡Ù†Ø¯Ø³ÙŠÙ†": "Ø£Ù†Øª Ø®Ø¨ÙŠØ± Ø¨Ø±Ù…Ø¬ÙŠØ§ØªØŒ ØªÙƒØªØ´Ù Ø§Ù„Ø«ØºØ±Ø§Øª ÙˆØªÙƒØªØ¨ Ø§Ù„Ø£ÙƒÙˆØ§Ø¯ Ø¨ÙƒÙØ§Ø¡Ø©."
}

# --- [2] Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…ØªØ·ÙˆØ±Ø© ---
def get_pdf_text(pdf_file):
    doc = fitz.open(stream=pdf_file.read(), filetype="pdf")
    return "".join([page.get_text() for page in doc])

def text_to_speech(text):
    tts = gTTS(text=text, lang='ar')
    fp = io.BytesIO()
    tts.write_to_fp(fp)
    return fp

# --- [3] Ø§Ù„Ù…Ø­Ø±Ùƒ Ø§Ù„ØªÙ†ÙÙŠØ°ÙŠ Ø§Ù„Ø¹Ø§Ø¨Ø± Ù„Ù„Ù‚Ø§Ø±Ø§Øª ---
def run_alliance_engine(prompt, image=None, pdf_text=None, audio_data=None):
    target_model = model_map[selected_model]
    system_instr = expert_map[selected_expert]
    
    try:
        if "Gemini" in selected_model:
            client = genai.Client(api_key=st.secrets["GEMINI_API_KEY"])
            content_list = [prompt]
            if image: content_list.append(image)
            if pdf_text: content_list.append(f"\nØ³ÙŠØ§Ù‚ Ù…Ù„Ù Ø§Ù„Ù€ PDF:\n{pdf_text}")
            if audio_data: content_list = [types.Part.from_bytes(data=audio_data, mime_type="audio/wav"), prompt]

            response = client.models.generate_content(
                model=target_model,
                contents=content_list,
                config=types.GenerateContentConfig(
                    system_instruction=system_instr,
                    tools=[types.Tool(google_search=types.GoogleSearch())] if live_search else None
                )
            )
            return response.text
        else:
            # ØªØ´ØºÙŠÙ„ Ø§Ù„Ø¹Ù‚ÙˆÙ„ Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠØ© Ø§Ù„Ø£Ø®Ø±Ù‰ Ø¹Ø¨Ø± OpenRouter
            client = OpenAI(base_url="https://openrouter.ai/api/v1", api_key=st.secrets["OPENROUTER_API_KEY"])
            res = client.chat.completions.create(
                model=target_model,
                messages=[{"role": "system", "content": system_instr}, {"role": "user", "content": prompt}]
            )
            return res.choices[0].message.content
    except Exception as e:
        return f"ğŸš¨ Ø¹Ø·Ù„ ØªÙ‚Ù†ÙŠ ÙÙŠ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø§Ù„Ø¹Ù‚ÙˆÙ„ Ø§Ù„Ø¯ÙˆÙ„ÙŠØ©: {str(e)}"

# --- [4] ÙˆØ§Ø¬Ù‡Ø© Ù…Ø±ÙƒØ² Ø§Ù„Ù‚ÙŠØ§Ø¯Ø© Ø§Ù„Ø³ÙŠØ§Ø¯ÙŠ ---
st.set_page_config(page_title="Ø¥Ù…Ø¨Ø±Ø§Ø·ÙˆØ±ÙŠØ© Ø§Ù„ØªØ­Ø§Ù„Ù 2026", layout="wide")
st.title("ğŸ›ï¸ Ø¥Ù…Ø¨Ø±Ø§Ø·ÙˆØ±ÙŠØ© Ø§Ù„ØªØ­Ø§Ù„Ù: Ù…Ø±ÙƒØ² Ø§Ù„Ù‚ÙˆÙ‰ Ø§Ù„Ø¹Ø¸Ù…Ù‰")

with st.sidebar:
    st.header("âš™ï¸ Ù„ÙˆØ­Ø© Ø§Ù„ØªØ­ÙƒÙ… Ø§Ù„Ø³ÙŠØ§Ø¯ÙŠØ©")
    selected_model = st.selectbox("Ø§Ù„Ø¹Ù‚Ù„ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ù…ØªÙˆÙØ±:", list(model_map.keys()))
    selected_expert = st.selectbox("Ø§Ù„Ù…Ø³ØªØ´Ø§Ø± Ø§Ù„Ù…ÙÙˆØ¶:", list(expert_map.keys()))
    live_search = st.toggle("Ø±Ø§Ø¯Ø§Ø± Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù„Ø­Ø¸ÙŠ ğŸ“¡", value=True)
    speak_out = st.toggle("Ù†Ø·Ù‚ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© (Ø¹Ø±Ø¨ÙŠ) ğŸ—£ï¸", value=False)
    
    st.divider()
    uploaded_file = st.file_uploader("Ø±ÙØ¹ (Image, PDF)", type=['png', 'jpg', 'jpeg', 'pdf'])
    
    st.write("ğŸ¤ Ø§Ù„Ø£Ù…Ø± Ø§Ù„ØµÙˆØªÙŠ:")
    audio = mic_recorder(start_prompt="Ø¨Ø¯Ø¡ Ø§Ù„ØªØ³Ø¬ÙŠÙ„", stop_prompt="Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø£Ù…Ø±", key='mic')

# --- [5] ØªÙ†ÙÙŠØ° Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© ---
if prompt := st.chat_input("Ø£ØµØ¯Ø± Ø£ÙˆØ§Ù…Ø±Ùƒ Ø§Ù„Ø³ÙŠØ§Ø¯ÙŠØ©..."):
    with st.chat_message("user"):
        st.markdown(prompt)
    
    with st.chat_message("assistant"):
        img = None
        pdf_txt = None
        
        if uploaded_file:
            if uploaded_file.type == "application/pdf":
                pdf_txt = get_pdf_text(uploaded_file)
                st.info("ğŸ“„ ØªÙ…Øª Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„ÙˆØ«ÙŠÙ‚Ø© Ø¨Ù†Ø¬Ø§Ø­")
            else:
                img = Image.open(uploaded_file)
                st.image(img, caption="ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…Ø±ÙÙ‚Ø©", width=300)

        with st.spinner("Ø¬Ø§Ø±ÙŠ Ø§Ø³ØªØ­Ø¶Ø§Ø± Ø§Ù„Ù‚ÙˆÙ‰ Ø§Ù„Ø¹Ø¸Ù…Ù‰..."):
            response = run_alliance_engine(prompt, image=img, pdf_text=pdf_txt, audio_data=audio['bytes'] if audio else None)
            st.markdown(response)
            
            if speak_out:
                audio_fp = text_to_speech(response)
                st.audio(audio_fp, format='audio/mp3')
