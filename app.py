import streamlit as st
import google.generativeai as genai
from openai import OpenAI
import io, os
from gtts import gTTS
from PIL import Image
from streamlit_mic_recorder import mic_recorder 

# --- 1. Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª ÙˆØ§Ù„ÙˆØ§Ø¬Ù‡Ø© (RTL) ---
st.set_page_config(page_title="Ù…Ù†ØµØ© Ù…ØµØ¹Ø¨ v16.12.0", layout="wide", page_icon="ğŸ¤")

st.markdown("""
    <style>
    .stApp { direction: rtl; text-align: right; }
    section[data-testid="stSidebar"] { direction: rtl; text-align: right; background-color: #111; }
    .stSelectbox label, .stSlider label { color: #00ffcc !important; font-weight: bold; }
    /* ØªÙ†Ø³ÙŠÙ‚ ØªØ¯ÙÙ‚ Ø§Ù„Ù†Øµ */
    .stChatMessage { transition: all 0.5s ease; }
    </style>
    """, unsafe_allow_html=True)

# Ø§Ù„Ø±Ø¨Ø· Ø§Ù„ØªÙ‚Ù†ÙŠ
api_key = st.secrets.get("GEMINI_API_KEY")
if api_key:
    genai.configure(api_key=api_key)

# --- 2. Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠØ© ---
with st.sidebar:
    st.header("ğŸ® Ù…Ø±ÙƒØ² Ø§Ù„ØªØ­ÙƒÙ… v16.12.0")
    
    # Ù…ÙŠØ²Ø© Ø§Ù„Ø¨Ø« Ù…Ù† Ø§Ù„Ø¥Ù†ØªØ±Ù†Øª (Ø¬Ø¯ÙŠØ¯)
    st.subheader("ğŸŒ Ø§Ù„Ø§ØªØµØ§Ù„ Ø§Ù„Ù…Ø¨Ø§Ø´Ø±")
    web_search_enabled = st.toggle("ØªÙØ¹ÙŠÙ„ Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ø¥Ù†ØªØ±Ù†Øª (Live)", value=True)
    
    st.divider()
    
    st.subheader("ğŸ¤ Ø§Ù„Ù…ØºØ±ÙÙˆÙ†")
    audio_record = mic_recorder(
        start_prompt="Ø¨Ø¯Ø¡ Ø§Ù„ØªØ³Ø¬ÙŠÙ„", 
        stop_prompt="Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„ØµÙˆØª", 
        just_once=True, 
        key='sidebar_mic'
    )
    
    st.divider()

    thinking_level = st.select_slider("ğŸ§  Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØªÙÙƒÙŠØ±:", options=["Low", "Medium", "High"], value="High")
    persona = st.selectbox("ğŸ‘¤ Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø®Ø¨ÙŠØ±:", ["Ø§Ù„Ù…Ø¹Ø±ÙÙˆÙ† (Ø£Ù‡Ù„ Ø§Ù„Ø¹Ù„Ù…)", "Ø®Ø¨ÙŠØ± Ø§Ù„Ù„ØºØ§Øª", "ÙˆÙƒÙŠÙ„ ØªÙ†ÙÙŠØ°ÙŠ", "Ù…Ø³Ø§Ø¹Ø¯ Ù…Ø¨Ø±Ù…Ø¬"])
    
    engine_choice = st.selectbox("ğŸ¯ Ø§Ù„Ù…Ø­Ø±Ùƒ:", ["gemini-2.0-flash", "gemini-1.5-pro"])
    uploaded_file = st.file_uploader("ğŸ“‚ Ø±ÙØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª:", type=["pdf", "csv", "txt", "jpg", "png", "jpeg"])
    
    if st.button("ğŸ—‘ï¸ Ù…Ø³Ø­ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©"):
        st.session_state.messages = []
        st.rerun()

# --- 3. Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø°Ø§ÙƒØ±Ø© ÙˆØ§Ù„Ø³ÙŠØ§Ù‚ (Context) ---
if "messages" not in st.session_state:
    st.session_state.messages = []

def get_history():
    """ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØªØ§Ø±ÙŠØ® Ù„ØªÙ†Ø³ÙŠÙ‚ Gemini"""
    history = []
    for msg in st.session_state.messages:
        role = "model" if msg["role"] == "assistant" else "user"
        history.append({"role": role, "parts": [msg["content"]]})
    return history

# Ø¹Ø±Ø¶ Ø§Ù„Ø±Ø³Ø§Ø¦Ù„ Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]): st.markdown(msg["content"])

# --- 4. Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª ÙˆØ§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…Ø¨Ø§Ø´Ø± ---
prompt = st.chat_input("Ø§Ø·Ù„Ø¨ Ø£Ø®Ø¨Ø§Ø± Ø§Ù„ÙŠÙˆÙ…ØŒ Ø§Ù„Ø·Ù‚Ø³ØŒ Ø£Ùˆ Ø£ÙŠ Ø³Ø¤Ø§Ù„ Ù…Ø¨Ø§Ø´Ø±...")

if prompt or audio_record or uploaded_file:
    user_txt = prompt if prompt else "ğŸ¤ [ØªÙ… Ø¥Ø±Ø³Ø§Ù„ Ø£Ù…Ø± ØµÙˆØªÙŠ]"
    st.session_state.messages.append({"role": "user", "content": user_txt})
    with st.chat_message("user"): st.markdown(user_txt)

    with st.chat_message("assistant"):
        try:
            # Ø¥Ø¹Ø¯Ø§Ø¯ Ù…ÙŠØ²Ø© Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…Ø¨Ø§Ø´Ø± (Google Search Tool)
            tools = [{"google_search_retrieval": {}}] if web_search_enabled else []
            
            model = genai.GenerativeModel(
                model_name=engine_choice,
                tools=tools
            )
            
            # Ø¨Ø¯Ø¡ Ù…Ø­Ø§Ø¯Ø«Ø© Ù…Ø¹ Ø§Ù„Ø³ÙŠØ§Ù‚ (Ù„ØªØ°ÙƒØ± Ù…Ø§ Ù‚ÙŠÙ„ Ø³Ø§Ø¨Ù‚Ø§Ù‹)
            chat = model.start_chat(history=get_history())
            
            # Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø·Ù„Ø¨ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
            full_prompt = f"Ø¨ØµÙØªÙƒ {persona} ÙˆØ¨Ù…Ø³ØªÙˆÙ‰ ØªÙÙƒÙŠØ± {thinking_level}: {user_txt}"
            
            # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙˆØ³Ø§Ø¦Ø· (ØµÙˆØ±/Ù…Ù„ÙØ§Øª)
            input_data = [full_prompt]
            if uploaded_file:
                if uploaded_file.type.startswith("image"):
                    input_data.append(Image.open(uploaded_file))
                else:
                    input_data.append(uploaded_file.read().decode())
            
            # ØªÙ†ÙÙŠØ° Ø§Ù„Ø¨Ø« Ø§Ù„Ù†ØµÙŠ (Streaming)
            response_placeholder = st.empty()
            full_response = ""
            
            # Ø¬Ù„Ø¨ Ø§Ù„Ø±Ø¯ Ø¨Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ¯ÙÙ‚
            response = chat.send_message(input_data, stream=True)
            
            for chunk in response:
                full_response += chunk.text
                response_placeholder.markdown(full_response + "â–Œ")
            
            response_placeholder.markdown(full_response)
            
            # Ù†Ø·Ù‚ Ø§Ù„Ø±Ø¯ Ø¢Ù„ÙŠØ§Ù‹
            if full_response:
                tts = gTTS(text=full_response[:300], lang='ar')
                audio_fp = io.BytesIO()
                tts.write_to_fp(audio_fp)
                st.audio(audio_fp, format='audio/mp3')
            
            # Ø­ÙØ¸ Ø§Ù„Ø±Ø¯ ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø©
            st.session_state.messages.append({"role": "assistant", "content": full_response})
            
        except Exception as e:
            st.error(f"ÙØ´Ù„ ÙÙŠ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©: {e}")
