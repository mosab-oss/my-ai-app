import streamlit as st
import google.generativeai as genai
from openai import OpenAI
import io, re
from gtts import gTTS
from PIL import Image

# 1. Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª ÙˆØ§Ù„Ø±Ø¨Ø·
st.set_page_config(page_title="Ù…Ù†ØµØ© Ù…ØµØ¹Ø¨ v16.3", layout="wide", page_icon="ğŸ’")

local_client = OpenAI(base_url="http://localhost:1234/v1", api_key="lm-studio")

api_key = st.secrets.get("GEMINI_API_KEY")
if api_key:
    genai.configure(api_key=api_key)

# 2. Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠØ© (Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù…Ø¯Ø±Ø³ Ø§Ù„Ù„ØºÙˆÙŠ)
with st.sidebar:
    st.header("ğŸ® Ù…Ø±ÙƒØ² Ø§Ù„ØªØ­ÙƒÙ…")
    
    engine_choice = st.selectbox(
        "ğŸ¯ Ø§Ø®ØªØ± Ø§Ù„Ù…Ø­Ø±Ùƒ:",
        ["DeepSeek R1 (Ù…Ø­Ù„ÙŠ)", "Gemini 2.5 Flash", "Gemma 3 27B", "Gemini 3 Pro"]
    )
    
    # Ù‡Ù†Ø§ Ø¹Ø§Ø¯ Ø§Ù„Ù…Ø¯Ø±Ø³ Ø§Ù„Ù„ØºÙˆÙŠ Ø§Ù„Ø°ÙŠ Ø³Ø£Ù„Øª Ø¹Ù†Ù‡
    persona = st.selectbox("ğŸ‘¤ Ø´Ø®ØµÙŠØ© Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯:", ["Ù…Ø¯Ø±Ø³ Ù„ØºÙˆÙŠ", "Ù…Ø³Ø§Ø¹Ø¯ Ù…Ø¨Ø±Ù…Ø¬", "Ù…Ø­Ù„Ù„ Ø°ÙƒÙŠ"])
    
    uploaded_file = st.file_uploader("ğŸ“¸ ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØ± (Ù„Ù€ Gemini):", type=["jpg", "png", "jpeg"])
    
    if st.button("ğŸ—‘ï¸ Ù…Ø³Ø­ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©"):
        st.session_state.messages = []
        st.rerun()

# 3. ØªÙ†Ø¸ÙŠÙ Ø±Ø¯ÙˆØ¯ DeepSeek (Ù„Ø­Ù„ Ù…Ø´ÙƒÙ„Ø© <think>)
def clean_response(text):
    return re.sub(r'<think>.*?</think>', '', text, flags=st.DOTALL).strip()

# 4. ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø¯Ø±Ø¯Ø´Ø©
if "messages" not in st.session_state:
    st.session_state.messages = []

for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(clean_response(msg["content"]))

# 5. Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
if prompt := st.chat_input("ØªØ­Ø¯Ø« Ù…Ø¹ Ø§Ù„Ù…Ø¯Ø±Ø³ Ø§Ù„Ù„ØºÙˆÙŠ Ø£Ùˆ Ø§Ù„Ù…Ø¨Ø±Ù…Ø¬..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        full_response = ""
        
        if "DeepSeek" in engine_choice:
            try:
                res = local_client.chat.completions.create(
                    model="deepseek-r1-distill-qwen-7b",
                    messages=[{"role": "system", "content": f"Ø£Ù†Øª {persona}"}, {"role": "user", "content": prompt}],
                    stream=True
                )
                placeholder = st.empty()
                for chunk in res:
                    if chunk.choices[0].delta.content:
                        full_response += chunk.choices[0].delta.content
                        placeholder.markdown(full_response + "â–Œ")
                placeholder.markdown(clean_response(full_response))
            except: st.error("ØªØ£ÙƒØ¯ Ù…Ù† ØªØ´ØºÙŠÙ„ LM Studio!")

        else: # Ù…Ø­Ø±ÙƒØ§Øª Ø¬ÙˆØ¬Ù„
            try:
                # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø§Ù„Ø£Ø­Ø¯Ø« Ù„ØªØ¬Ù†Ø¨ Ø®Ø·Ø£ 404
                model_name = "gemini-1.5-flash-latest" 
                model = genai.GenerativeModel(model_name)
                res = model.generate_content([prompt, Image.open(uploaded_file)] if uploaded_file else prompt)
                full_response = res.text
                st.markdown(full_response)
            except Exception as e:
                st.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø§ØªØµØ§Ù„: {e}")

        # Ø§Ù„ØµÙˆØª
        if full_response:
            try:
                tts = gTTS(text=clean_response(full_response)[:300], lang='ar')
                audio_io = io.BytesIO()
                tts.write_to_fp(audio_io)
                st.audio(audio_io)
            except: pass
            st.session_state.messages.append({"role": "assistant", "content": full_response})
