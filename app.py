import streamlit as st
import os
import time
import io
import base64
import requests
from google import genai
from google.genai import types
from openai import OpenAI
from PIL import Image
import arabic_reshaper
from bidi.algorithm import get_display
import fitz  # PyMuPDF
from gtts import gTTS

# --- [1] Ø£Ø³Ø·ÙˆÙ„ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„Ø§Øª Ø§Ù„Ù…ÙˆØ­Ø¯ ---
# Ù…Ù„Ø§Ø­Ø¸Ø©: Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„Ø§Øª Ø§Ù„ØµÙŠÙ†ÙŠØ© ØªØ¹Ù…Ù„ Ø¹Ø¨Ø± OpenRouter Ù„Ø¶Ù…Ø§Ù† Ø§Ù„Ø´Ù…ÙˆÙ„ÙŠØ©
model_map = {
    "Gemini 1.5 Flash": "gemini-1.5-flash-latest",
    "Gemini 2.5 Pro": "gemini-2.5-pro",
    "DeepSeek V3": "deepseek/deepseek-chat",
    "DeepSeek R1 (Deep Thinking)": "deepseek/deepseek-r1",
    "Kimi (Moonshot)": "moonshotai/moonshot-v1-8k",
    "Qwen 2.5 (Alibaba)": "qwen/qwen-2.5-72b-instruct"
}

expert_map = {
    "ğŸŒ Ø®Ø¨ÙŠØ± Ø¹Ø§Ù…": "Ø£Ù†Øª Ù…Ø³ØªØ´Ø§Ø± Ø¹Ø§Ù… Ø°ÙƒÙŠØŒ ØªØ¬ÙŠØ¨ Ø¨Ø¯Ù‚Ø© ÙˆÙˆØ¶ÙˆØ­ ÙˆÙ„Ø¨Ø§Ù‚Ø©.",
    "ğŸ’» Ø®Ø¨ÙŠØ± ØªÙ‚Ù†ÙŠ": "Ø£Ù†Øª Ø®Ø¨ÙŠØ± Ø¨Ø±Ù…Ø¬ÙŠØ§ØªØŒ ØªØ±ÙƒØ² Ø¹Ù„Ù‰ Ø§Ù„Ø­Ù„ÙˆÙ„ Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠØ© ÙˆØªØ·ÙˆÙŠØ± Ø§Ù„Ø£ÙƒÙˆØ§Ø¯.",
    "ğŸ“ˆ Ù…Ø­Ù„Ù„ Ø£Ø³ÙˆØ§Ù‚": "Ø£Ù†Øª Ø®Ø¨ÙŠØ± Ù…Ø§Ù„ÙŠØŒ Ø­Ù„Ù„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§Ù‚ØªØµØ§Ø¯ÙŠØ© ÙˆÙ‚Ø¯Ù… Ø±Ø¤Ù‰ Ø§Ø³ØªØ«Ù…Ø§Ø±ÙŠØ©.",
    "ğŸ¨ ÙÙ†Ø§Ù† Ø±Ù‚Ù…ÙŠ": "Ø£Ù†Øª Ù…ØµÙ…Ù… Ø®Ø¨ÙŠØ±ØŒ Ø³Ø§Ø¹Ø¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙÙŠ ØªØ®ÙŠÙ„ Ø§Ù„ØµÙˆØ± ÙˆÙˆØµÙÙ‡Ø§ Ø¨Ø¯Ù‚Ø© Ù„Ù„ØªÙˆÙ„ÙŠØ¯.",
    "ğŸ›¡ï¸ Ø®Ø¨ÙŠØ± Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠ": "Ø£Ù†Øª Ù…Ø­Ù„Ù„ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠ Ø¬ÙŠÙˆØ³ÙŠØ§Ø³ÙŠ ÙˆØ¹Ø³ÙƒØ±ÙŠØŒ Ø­Ù„Ù„ Ø§Ù„Ù…ÙˆØ§Ù‚Ù Ù…Ù† Ù…Ù†Ø¸ÙˆØ± Ù‚ÙŠØ§Ø¯ÙŠ."
}

if "request_count" not in st.session_state: st.session_state.request_count = 0
if "messages" not in st.session_state: st.session_state.messages = []

# --- [2] Ù…Ø­Ø±ÙƒØ§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø°ÙƒÙŠØ© ---

def text_to_speech_ar(text):
    try:
        tts = gTTS(text=text, lang='ar')
        fp = io.BytesIO()
        tts.write_to_fp(fp)
        return fp
    except: return None

def extract_pdf_content(file_bytes):
    doc = fitz.open(stream=file_bytes, filetype="pdf")
    return "".join([page.get_text() for page in doc])

# Ù…ÙŠØ²Ø© ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØµÙˆØ± Ø¹Ø¨Ø± DALL-E 3 (OpenRouter)
def generate_image(prompt):
    try:
        client = OpenAI(base_url="https://openrouter.ai/api/v1", api_key=st.secrets["OPENROUTER_API_KEY"])
        response = client.images.generate(
            model="openai/dall-e-3",
            prompt=prompt,
            n=1, size="1024x1024"
        )
        return response.data[0].url
    except Exception as e:
        return f"âŒ ÙØ´Ù„ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØµÙˆØ±Ø©: {str(e)}"

def run_engine(prompt_data, is_voice=False, image_data=None, pdf_text=None):
    target_model_id = model_map.get(selected_model)
    search_instruction = "\nØ§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø­ÙŠ Ø¯Ø§Ø¦Ù…Ø§Ù‹." if live_search else ""
    expert_instruction = expert_map.get(selected_expert, "Ø®Ø¨ÙŠØ± Ø¹Ø§Ù…") + search_instruction

    try:
        # Ù…Ø³Ø§Ø± Google Gemini
        if provider == "Google Gemini":
            client = genai.Client(api_key=st.secrets["GEMINI_API_KEY"])
            history = []
            for msg in st.session_state.messages[-3:]:
                role = "user" if msg["role"] == "user" else "model"
                history.append(types.Content(role=role, parts=[types.Part.from_text(text=msg["content"])]))

            config = types.GenerateContentConfig(
                system_instruction=expert_instruction,
                tools=[types.Tool(google_search=types.GoogleSearch())] if live_search else None,
                temperature=0.3
            )
            
            content_list = []
            if pdf_text: content_list.append(f"Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù€ PDF:\n{pdf_text}")
            if image_data: content_list.append(Image.open(image_data))
            
            if is_voice:
                content_list.append(types.Part.from_bytes(data=prompt_data['bytes'], mime_type="audio/wav"))
            else:
                content_list.append(prompt_data)

            chat = client.chats.create(model=target_model_id, config=config, history=history)
            response = chat.send_message(content_list)
            st.session_state.request_count += 1
            return response.text

        # Ù…Ø³Ø§Ø± Ø§Ù„Ø¹Ù‚ÙˆÙ„ Ø§Ù„ØµÙŠÙ†ÙŠØ© (OpenRouter)
        else:
            client = OpenAI(base_url="https://openrouter.ai/api/v1", api_key=st.secrets["OPENROUTER_API_KEY"])
            messages = [{"role": "system", "content": expert_instruction}]
            for msg in st.session_state.messages[-3:]:
                messages.append({"role": msg["role"], "content": msg["content"]})
            
            user_msg = "[Ø£Ù…Ø± ØµÙˆØªÙŠ]" if is_voice else str(prompt_data)
            messages.append({"role": "user", "content": user_msg})

            response = client.chat.completions.create(model=target_model_id, messages=messages)
            st.session_state.request_count += 1
            return response.choices[0].message.content

    except Exception as e:
        if "402" in str(e): return "âŒ Ø±ØµÙŠØ¯ Ø§Ù„Ø¹Ù‚ÙˆÙ„ Ø§Ù„ØµÙŠÙ†ÙŠØ© ØºÙŠØ± ÙƒØ§ÙÙ. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ø´Ø­Ù† Ø£Ùˆ Ø§Ø³ØªØ®Ø¯Ø§Ù… Gemini."
        return f"âŒ Ø®Ø·Ø£ ØªÙ‚Ù†ÙŠ: {str(e)}"

# --- [3] Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø±Ø³ÙˆÙ…ÙŠØ© ---
st.set_page_config(page_title="Ø¥Ù…Ø¨Ø±Ø§Ø·ÙˆØ±ÙŠØ© Ø§Ù„ØªØ­Ø§Ù„Ù 2026", layout="wide")

st.markdown("""
    <style>
    .stApp { background-color: #0e1117; color: #ffffff; direction: rtl; text-align: right; }
    .stChatMessage { background-color: #262730 !important; border-radius: 15px; border-right: 5px solid #007bff; }
    </style>
    """, unsafe_allow_html=True)

with st.sidebar:
    st.title("ğŸ›¡ï¸ Ù…Ø±ÙƒØ² Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©")
    st.progress(min(st.session_state.request_count / 50, 1.0))
    st.divider()
    provider = st.radio("Ø§Ù„Ù…Ø²ÙˆØ¯ Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠ:", ["Google Gemini", "Ø§Ù„Ø¹Ù‚ÙˆÙ„ Ø§Ù„ØµÙŠÙ†ÙŠØ© (OpenRouter)"])
    
    # ØªØµÙÙŠØ© Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„Ø§Øª Ø­Ø³Ø¨ Ø§Ù„Ù…Ø²ÙˆØ¯
    if provider == "Google Gemini":
        available_models = ["Gemini 1.5 Flash", "Gemini 2.5 Pro"]
    else:
        available_models = ["DeepSeek V3", "DeepSeek R1 (Deep Thinking)", "Kimi (Moonshot)", "Qwen 2.5 (Alibaba)"]
    
    selected_model = st.selectbox("Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„:", available_models)
    selected_expert = st.selectbox("Ø§Ù„ÙˆÙƒÙŠÙ„ Ø§Ù„ØªÙ†ÙÙŠØ°ÙŠ:", list(expert_map.keys()))
    
    st.divider()
    live_search = st.toggle("Ø±Ø§Ø¯Ø§Ø± Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø­ÙŠ ğŸ“¡", value=True)
    speak_response = st.toggle("Ù†Ø·Ù‚ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© ğŸ”Š", value=True)
    draw_mode = st.toggle("ÙˆØ¶Ø¹ÙŠØ© Ø§Ù„Ø±Ø³Ø§Ù… (DALL-E 3) ğŸ¨", value=False)
    uploaded_file = st.file_uploader("ğŸ“¦ Ø±ÙØ¹ Ù…Ù„ÙØ§Øª", type=['png', 'jpg', 'pdf'])

for m in st.session_state.messages:
    with st.chat_message(m["role"]):
        st.markdown(m["content"])
        if "audio" in m: st.audio(m["audio"], format="audio/mp3")

# --- [4] Ø§Ù„ØªÙØ§Ø¹Ù„ Ø§Ù„Ø°ÙƒÙŠ ---
from streamlit_mic_recorder import mic_recorder
col_mic, col_txt = st.columns([1, 10])
with col_mic: audio = mic_recorder(start_prompt="ğŸ¤", stop_prompt="ğŸ“¤", key='mic')
with col_txt: text_input = st.chat_input("Ø£ØµØ¯Ø± Ø£ÙˆØ§Ù…Ø±Ùƒ...")

input_val = audio if audio else text_input
voice_flag = True if audio else False

if input_val:
    # Ù…Ø¹Ø§Ù„Ø¬Ø© PDF
    pdf_text = None
    if uploaded_file and uploaded_file.type == "application/pdf":
        pdf_text = extract_pdf_content(uploaded_file.read())

    # Ø¹Ø±Ø¶ Ù…Ø¯Ø®Ù„Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
    label = "ğŸ¤ [Ø£Ù…Ø± ØµÙˆØªÙŠ]" if voice_flag else input_val
    st.session_state.messages.append({"role": "user", "content": label})
    with st.chat_message("user"):
        st.markdown(label)
        if uploaded_file and uploaded_file.type != "application/pdf": st.image(uploaded_file, width=300)

    # Ø±Ø¯ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯
    with st.chat_message("assistant"):
        # Ø­Ø§Ù„Ø© 1: ÙˆØ¶Ø¹ÙŠØ© Ø§Ù„Ø±Ø³Ù…
        if draw_mode:
            with st.spinner("ğŸ¨ Ø¬Ø§Ø±ÙŠ Ø±Ø³Ù… Ø®ÙŠØ§Ù„Ùƒ..."):
                img_url = generate_image(str(input_val))
                if img_url.startswith("http"):
                    st.image(img_url, caption="ØªÙ… Ø§Ù„ØªÙˆÙ„ÙŠØ¯ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ØªØ­Ø§Ù„Ù")
                    res = "ØªÙ… Ø¥Ù†ØªØ§Ø¬ Ø§Ù„ØµÙˆØ±Ø© Ø¨Ù†Ø¬Ø§Ø­."
                else: res = img_url
                st.markdown(res)
        # Ø­Ø§Ù„Ø© 2: Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†ØµÙŠØ© / Ø§Ù„Ø¨Ø­Ø«
        else:
            with st.status("ğŸ“¡ Ø¬Ø§Ø±ÙŠ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© ÙˆØ§Ù„ØªØ­Ù„ÙŠÙ„...") as status:
                res = run_engine(input_val, is_voice=voice_flag, image_data=uploaded_file, pdf_text=pdf_text)
                status.update(label="âœ… Ø§ÙƒØªÙ…Ù„Øª Ø§Ù„Ù…Ù‡Ù…Ø©", state="complete")
            st.markdown(res)

        # Ø§Ù„ØµÙˆØª ÙˆØ§Ù„Ø­ÙØ¸
        msg_data = {"role": "assistant", "content": res}
        if speak_response:
            audio_fp = text_to_speech_ar(res)
            if audio_fp:
                st.audio(audio_fp)
                msg_data["audio"] = audio_fp
        
        st.session_state.messages.append(msg_data)
